{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": ""
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sklearn as sk\n",
                "\n",
                "import torch\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "# Feel free to tune these hyperparameters, \n",
                "# as long as they are kept optimal/fair for both base and improved model\n",
                "batch_size = 256\n",
                "epochs = 10"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset and DataLoaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Files already downloaded and verified\n"
                }
            ],
            "source": [
                "######################################################################\n",
                "# Create the Dataset and DataLoader (both for training and testing)\n",
                "######################################################################\n",
                "# Hint: three things are typically done to the data \n",
                "#   1. create the data transform object for the training/testing data\n",
                "#   2. create the dataset object\n",
                "#   3. create the dataloader object\n",
                "######################################################################\n",
                "# Here is a simple code structure for the CIFAR10 training dataset, please complete the following code.\n",
                "# train_transform = transforms.Compose([???])\n",
                "# train_dataset = torchvision.datasets.CIFAR10(???)\n",
                "# train_loader = torch.utils.data.DataLoader(???)\n",
                "#######################################################################\n",
                "\n",
                "# for the training and test transform, here we only need two transforms:\n",
                "#   1. convert the image to tensor\n",
                "#   2. normalize the tensor with mean=(0.5, 0.5, 0.5) and variance=(0.5, 0.5, 0.5)\n",
                "pass\n",
                "\n",
                "# create the training and test dataset with torchvision.datasets.CIFAR10() class.\n",
                "pass\n",
                "\n",
                "# create the training and test dataloader with torch.utils.data.DataLoader() class.\n",
                "pass\n",
                "\n",
                "# the 10 classes in the CIFAR10 dataset.\n",
                "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper function to evaluate the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import classification_report\n",
                "\n",
                "def clf_report(model, dataloader):\n",
                "    \"\"\"Our implementation of the classfication report.\"\"\"\n",
                "    preds, labels = [], []\n",
                "    # \"with torch.no_grad()\" helps the model to run faster, \n",
                "    # as it does not need to calculate the gradients.\n",
                "    with torch.no_grad():\n",
                "        for data in iter(dataloader):\n",
                "            x, label = data[0], data[1]\n",
                "            \n",
                "            # forward the output (logits) of the model\n",
                "            pass\n",
                "            # get the prediction (a scaler representing the label)\n",
                "            pass\n",
                "            # append the prediction and label to the list\n",
                "            pass\n",
                "\n",
                "    preds = np.hstack(preds)\n",
                "    labels = np.hstack(labels)\n",
                "\n",
                "    return classification_report(labels, preds)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Baseline Model\n",
                "This model severely suffers from the problem of overfitting. Setting epochs=5 will help, but it doesn't really solve the essential problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Base Model Definition\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class BaseModel(nn.Module):\n",
                "    \"\"\"\n",
                "    Structure of the model: \n",
                "        1. Conv2D layer with (in_channels=3, out_channels=6, kernel_size=5)\n",
                "        2. ReLU activation\n",
                "        3. Max Pooling layer with (kernel_size=2, stride=2)\n",
                "        4. Conv2D layer with (in_channels=6, out_channels=16, kernel_size=5)\n",
                "        5. ReLU activation\n",
                "        6. Max Pooling layer with (kernel_size=2, stride=2)\n",
                "        7. Flatten layer\n",
                "        8. Fully connected layer with (in_features=???, out_features=120)\n",
                "        9. ReLU activation\n",
                "        10. Fully connected layer with (in_features=???, out_features=84)\n",
                "        11. ReLU activation\n",
                "        12. Fully connected layer with (in_features=???, out_features=???) # the output size is the number of classes\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        pass\n",
                "\n",
                "    def forward(self, x):\n",
                "        pass\n",
                "\n",
                "# create the model and move it to GPU for training acceleration\n",
                "# base_model = ???\n",
                "pass\n",
                "\n",
                "# create the criterion, which should be ??? for multi-class classification?\n",
                "# criterion = ???\n",
                "pass\n",
                "\n",
                "# create the optimizer:\n",
                "# SGD optimizer with learning rate 0.001 and momentum 0.9\n",
                "# optimizer = ???\n",
                "pass\n",
                "\n",
                "# Network Training\n",
                "from tqdm import tqdm\n",
                "\n",
                "for epoch in range(epochs):  # loop over the dataset multiple times\n",
                "\n",
                "    running_loss = 0.0\n",
                "    for i, data in enumerate(tqdm(trainloader), 0):\n",
                "        # get the inputs; data is a list of [inputs, labels]\n",
                "        inputs, labels = data[0], data[1]\n",
                "\n",
                "        # zero the parameter gradients\n",
                "        pass\n",
                "\n",
                "        # forward the output\n",
                "        pass\n",
                "\n",
                "        # compute the loss with the criterion object\n",
                "        pass\n",
                "\n",
                "        # backward the loss\n",
                "        pass\n",
                "\n",
                "        # update the model\n",
                "        pass\n",
                "\n",
                "        # TODO: log the training losses, accuracies\n",
                "        pass\n",
                "\n",
                "        # TODO: log the validation losess, accuracies\n",
                "        pass\n",
                "\n",
                "# after training, let's evaluate the model on the test dataset, \n",
                "# with our previously implemented \"clf_report\" function.\n",
                "# print('Base Model on Test Data:\\n', clf_report(???, ???))\n",
                "pass"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Designing Improved CNN model\n",
                "We increase the number of Conv Layers (going deeper) and reduces the number of the parameters in FC layers, which combind alleviate the problem of overfitting.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Improved CNN model\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "\n",
                "class ImprovedModel(nn.Module):\n",
                "    \"\"\"\n",
                "    It's time for you to practice the \"well-established rules\" of designing CNN models. \n",
                "    The only goal of this section, is to design a CNN model that can achieve a higher accuracy than the base model.\n",
                "    Here are some tips for you:\n",
                "        1. deeper network\n",
                "        2. smaller kernel size and deep network\n",
                "        3. batch-norm\n",
                "        4. dropout\n",
                "        5. skip-connection, even?\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        pass\n",
                "\n",
                "    def forward(self, x):\n",
                "        pass\n",
                "\n",
                "# put the model to the GPU\n",
                "imp_model = ImprovedModel()\n",
                "\n",
                "# (same???) optimizer and loss definition as the previous cell.\n",
                "# criterion = ???\n",
                "# optimizer = ???\n",
                "\n",
                "# Network Training\n",
                "from tqdm import tqdm\n",
                "\n",
                "for epoch in range(epochs):  # loop over the dataset multiple times\n",
                "\n",
                "    running_loss = 0.0\n",
                "    for i, data in enumerate(tqdm(trainloader), 0):\n",
                "        # get the inputs; data is a list of [inputs, labels]\n",
                "        inputs, labels = data[0], data[1]\n",
                "\n",
                "        # same training process as the previous cell,\n",
                "        # can you implement it without referring to the hints in the previous cell?\n",
                "        pass\n",
                "\n",
                "        # log the training losses, accuracies\n",
                "        pass\n",
                "\n",
                "        # log the validation losess, accuracies\n",
                "        pass"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pre-trained CNNs for Transfer Learning\n",
                "Now we are going to use the pre-trained neural network on a larger-scale dataset to do transfer learning on the current cifar10 dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "######################################################################\n",
                "# Create the Dataset and DataLoader (both for training and testing)\n",
                "# Note: **DIFFERENT FROM THE PREVIOUS CELL!!!**\n",
                "######################################################################\n",
                "\n",
                "# for the training and test transform, we need to consider what \n",
                "# the previous transform did to the data with performing the pre-training.\n",
                "#   1. Resize the original CIFAR10 image to (224x224x3), since the pre-trained model requires this size.\n",
                "#   2. normalize the tensor with mean=[0.485, 0.456, 0.406] and variance=[0.229, 0.224, 0.225]\n",
                "pass\n",
                "\n",
                "# create the training and test dataset with torchvision.datasets.CIFAR10() class.\n",
                "pass\n",
                "\n",
                "# create the training and test dataloader with torch.utils.data.DataLoader() class.\n",
                "pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "######################################################################\n",
                "# Create the pre-trained model and fix (some of) its parameters,\n",
                "# Here we use ResNet18 network as the pre-trained model.\n",
                "######################################################################\n",
                "\n",
                "# import the pre-trained model from torchvision.models\n",
                "pass\n",
                "\n",
                "# instantiate the model\n",
                "pass \n",
                "\n",
                "# fixed the parameters of the pre-trained model using\n",
                "# .requires_grad = False for all parameters\n",
                "pass\n",
                "\n",
                "# brute-forcely change the last layer of the pre-trained model to adapt to the CIFAR10 dataset.\n",
                "# To be specific, the last layer of the pre-trained model is a fully connected layer with 1000 output features,\n",
                "# so replace it with 10 output features.\n",
                "pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# (same???) optimizer and loss definition as the previous cell.\n",
                "# criterion = ???\n",
                "# optimizer = ???\n",
                "\n",
                "# Network Training\n",
                "from tqdm import tqdm\n",
                "\n",
                "for epoch in range(epochs):  # loop over the dataset multiple times\n",
                "\n",
                "    running_loss = 0.0\n",
                "    for i, data in enumerate(tqdm(???), 0):\n",
                "        # get the inputs; data is a list of [inputs, labels]\n",
                "        inputs, labels = data[0], data[1]\n",
                "\n",
                "        # same training process as the previous cell,\n",
                "        # can you implement it without referring to the hints in the previous cell?\n",
                "        pass\n",
                "\n",
                "        # log the training losses, accuracies\n",
                "        pass\n",
                "\n",
                "        # log the validation losess, accuracies\n",
                "        pass"
            ]
        }
    ]
}
